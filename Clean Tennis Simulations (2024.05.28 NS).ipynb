{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdd4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from janitor import pivot_longer,pivot_wider\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats # for truncated normal distribution\n",
    "import sklearn.model_selection # for train/test split and also model\n",
    "import pickle # save model \n",
    "from sklearn import preprocessing # standardize inputs for neural network\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59fe2d",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "This section provides some helper functions for work with the truncated normal distribution and with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e13494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated normal function scaling a and b first\n",
    "def trunc_normal(a_trunc,b_trunc,mu,sigma):\n",
    "    a, b = (a_trunc - mu) / sigma, (b_trunc - mu) / sigma\n",
    "    dist = stats.truncnorm(a=a,b=b,loc = mu,scale = sigma)\n",
    "    return dist\n",
    "\n",
    "# return negative log likelihood given certain parameters for tuncated normal distribution\n",
    "def log_lik_trunc_normal(data,mu_test,sigma_int = .01,a_trunc = 0,b_trunc = 1):\n",
    "    a_scaled = (a_trunc-mu_test)/sigma_int\n",
    "    b_scaled = (b_trunc-mu_test)/sigma_int\n",
    "    out = stats.truncnorm.nnlf(\n",
    "        [\n",
    "            a_scaled,\n",
    "            b_scaled,\n",
    "            mu_test,\n",
    "            sigma_int\n",
    "        ],\n",
    "        data\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# distinct() function from R, assing to method for method chaining\n",
    "def distinct_df(self,vars_to_distinct = 'Everything'):\n",
    "    if vars_to_distinct == 'Everything':\n",
    "        out = self.groupby(\n",
    "            [i for i in self.columns],\n",
    "            as_index = False\n",
    "        ).size().drop('size',axis = 1)\n",
    "    else:\n",
    "        out = self.groupby(\n",
    "            vars_to_distinct,\n",
    "            as_index = False\n",
    "        ).size().drop('size',axis = 1)\n",
    "    return out\n",
    "\n",
    "def distinct(self):\n",
    "    return pd.unique(self)\n",
    "\n",
    "pd.Series.distinct = pd.unique\n",
    "\n",
    "pd.DataFrame.distinct = distinct_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b1b80",
   "metadata": {},
   "source": [
    "# Load Data and Add Features\n",
    "There are some miscellaneous fields added here that were considered for use in this or related analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matches 2003-2023\n",
    "years = range(2003,2024)\n",
    "match_datasets = [\n",
    "    pd.read_csv(f\"https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master/atp_matches_{i}.csv\") for i in years\n",
    "]\n",
    "matches = pd.concat(match_datasets)\n",
    "\n",
    "matches = matches.assign(\n",
    "    match_id = matches['tourney_date'].astype('str') + '-' + matches['winner_id'].astype(str) + \n",
    "        '-' + matches['loser_id'].astype(str),\n",
    "    year = list(map(lambda x: x[0:4],matches['tourney_date'].astype('str')))\n",
    ").rename(\n",
    "    columns = lambda x: x.replace('1st','first').replace('2nd','second').replace('l_','loser_'),\n",
    "    inplace = False\n",
    ").rename(\n",
    "    columns = lambda x: x if x == 'draw_size' else x.replace('w_','winner_')\n",
    ").assign(\n",
    "    num_tiebreaks = lambda df: list(\n",
    "        map(\n",
    "            lambda x: len(re.findall('\\\\(',x)),\n",
    "            df['score']\n",
    "        )\n",
    "    ),\n",
    "    winner_tiebreaks_won = lambda df: list(\n",
    "        map(\n",
    "            lambda x: len(re.findall('7-6\\\\(',x)),\n",
    "            df['score']\n",
    "        )\n",
    "    ),\n",
    "    loser_tiebreaks_won = lambda df: list(\n",
    "        map(\n",
    "            lambda x: len(re.findall('6-7\\\\(',x)),\n",
    "            df['score']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "w_l_regex = re.compile('(winner|loser)')\n",
    "\n",
    "# filter out retirements, no sv games\n",
    "def not_retirement(score):\n",
    "    out = [not 'RET' in i for i in score]\n",
    "    return out\n",
    "\n",
    "player_matches = matches.pivot_longer( # get player_matches\n",
    "    column_names = [x for x in matches.columns if len(re.findall(w_l_regex,x)) != 0],\n",
    "    names_to = ['result','.value'],\n",
    "    names_pattern = r'(winner|loser)_(.+)'    \n",
    ").rename( # convert to snake case\n",
    "    columns = lambda x: re.sub(r'(\\w)([A-Z])',r'\\g<1>_\\g<2>',x).lower()\n",
    ").query( # no retirements, no walkovers, no missing sv_gms or svpt \n",
    "    '@not_retirement(score) and score != \"W/O\" and sv_gms != 0 and svpt != 0'\n",
    ").query(# no carpet matches, no missing surface, no missing match stats, no missing age, no missing rank\n",
    "    'surface != \"Carpet\" and not @pd.isna(surface) and not @pd.isna(ace)\\\n",
    "    and not @pd.isna(age) and not @pd.isna(rank)'\n",
    ").fillna( # how to deal with NAs\n",
    "    value = {\n",
    "        'seed':'Unseeded', # treat unseeded as different category\n",
    "        'entry':'NA', # treat no entry as separate, to create dummies without NA column\n",
    "        'minutes':1e9, # placeholder, not going to include in model\n",
    "        'hand':'U', # fill unknown for hand\n",
    "        'ht': 'Unknown' # dont care about missing height\n",
    "    }\n",
    ").query( # remove odd hand entries and rounds that can't be ordered\n",
    "    'hand not in [\"A\",\"U\"] and round not in [\"ER\",\"RR\"]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5978acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a bunch of features\n",
    "intermediate_pm = player_matches.assign(\n",
    "    rally_svpt = lambda df: df.svpt - df.ace - df.df,\n",
    "    first_lost = lambda df: df.first_in - df.first_won,\n",
    "    second_lost = lambda df: df.svpt - df.first_in - df.second_won,\n",
    "    first_in_per = lambda df: df.first_in/df.svpt,\n",
    "    first_won_per = lambda df: df.first_won/df.first_in,\n",
    "    second_won_per = lambda df: df.second_won/(df.second_won+df.second_lost),\n",
    "    hold_per = lambda df: (df.sv_gms - (df.bp_faced - df.bp_saved))/df.sv_gms,\n",
    "    svptw = lambda df: df.first_won + df.second_won,\n",
    "    svptw_per = lambda df: df.svptw/df.svpt,\n",
    "    rally_svptw = lambda df: df.first_won + df.second_won - df.ace,\n",
    "    rally_svptw_per = lambda df: df.rally_svptw/df.rally_svpt,\n",
    "    pts_lost_per_svgm = lambda df: (df.first_lost + df.second_lost)/df.sv_gms,\n",
    "    ace_per = lambda df: df.ace/df.svpt,\n",
    "    df_per = lambda df: df['df']/df.svpt\n",
    ").dropna() # a couple of issues where there are no second serve points\n",
    "\n",
    "\n",
    "# function to change tourney_level\n",
    "def change_tourney_level(tourney_level_og):\n",
    "    if tourney_level_og == \"M\":\n",
    "        return \"Masters\"\n",
    "    elif tourney_level_og == \"A\":\n",
    "        return \"ATP 250/500\"\n",
    "    elif tourney_level_og == \"G\":\n",
    "        return \"Grand Slam\"\n",
    "    elif tourney_level_og == \"F\":\n",
    "        return \"Year End Final\"\n",
    "    elif tourney_level_og == \"D\":\n",
    "        return \"Davis Cup\"\n",
    "    \n",
    "# function to bin rankings\n",
    "def bin_rankings(x):\n",
    "    if x <= 10:\n",
    "        return \"Top 10\"\n",
    "    elif x<= 25:\n",
    "        return \"Top 25\"\n",
    "    elif x <= 50:\n",
    "        return \"Top 50\"\n",
    "    elif x <= 100:\n",
    "        return \"Top 100\"\n",
    "    else:\n",
    "        return \"Outside Top 100\"\n",
    "\n",
    "\n",
    "# player_matches_wfeat\n",
    "player_matches_wfeat = intermediate_pm.merge(\n",
    "    intermediate_pm.rename(\n",
    "        columns = lambda x: x if x == 'match_id' else 'opp_' + x,\n",
    "        inplace = False\n",
    "    ),\n",
    "    how = 'left',\n",
    "    on = 'match_id'\n",
    ").query(\n",
    "    'name != opp_name'\n",
    ").assign(\n",
    "    rtgms = lambda df: df.opp_sv_gms,\n",
    "    aces_against = lambda df: df.opp_ace,\n",
    "    break_per = lambda df: 1- df.opp_hold_per,\n",
    "    bp_created = lambda df: df.opp_bp_faced,\n",
    "    bp_conversion = lambda df: 1-(df.opp_bp_saved/df.opp_bp_faced),\n",
    "    bp_per_gm = lambda df: df.bp_created/df.rtgms,\n",
    "    rpts = lambda df: df.opp_svpt,\n",
    "    rpw = lambda df: df.opp_svpt - df.opp_first_won - df.opp_second_won,\n",
    "    rpw_per = lambda df: df.rpw/df.rpts,\n",
    "    first_rpw_per = lambda df: 1-df.opp_first_won_per,\n",
    "    second_rpw_per = lambda df: 1-df.opp_second_won_per,\n",
    "    rally_rp = lambda df: df.opp_rally_svpt,\n",
    "    rally_rpw = lambda df: df.rally_rp - df.opp_rally_svptw,\n",
    "    rally_rpw_per = lambda df: df.rally_rpw/df.rally_rp,\n",
    "    dominance_ratio = lambda df: df.rpw_per/(1-df.svptw_per),\n",
    "    bp_face_freq = lambda df: df.bp_faced/df.svpt,\n",
    "    bp_create_freq = lambda df: df.bp_created/df.rpts,\n",
    "    ace_per_against = lambda df: df.opp_ace/df.opp_svpt\n",
    ").filter(\n",
    "    regex=r'^(?!opp).+',\n",
    "    axis = 1\n",
    ").merge(\n",
    "    intermediate_pm.rename(\n",
    "        columns = lambda x: x if x == 'match_id' else 'opp_' + x,\n",
    "        inplace = False\n",
    "    ).loc[\n",
    "        :,\n",
    "        ['match_id','opp_name','opp_rank','opp_rank_points']\n",
    "    ],\n",
    "    how = 'left',\n",
    "    on = 'match_id'\n",
    "\n",
    ").query(\n",
    "    'name != opp_name'\n",
    ").assign(\n",
    "    rank_bin = lambda df: list(map(bin_rankings,df['rank'])),\n",
    "    opp_rank_bin = lambda df: list(map(bin_rankings,df.opp_rank)),\n",
    "    tourney_level = lambda df: list(map(change_tourney_level,df['tourney_level'])),\n",
    "    # fix data issue with entry \n",
    "    entry = lambda df: list(map(lambda x: x if x != \"Alt\" else \"ALT\",df['entry'])),\n",
    "    result = lambda df: list(\n",
    "        map(\n",
    "            lambda x: int(x == 'winner'),\n",
    "            df['result']\n",
    "        )\n",
    "    ),\n",
    "    year = lambda df: df['year'].astype(float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feats = [\n",
    "    # id first and outcome\n",
    "    'match_id', 'result',\n",
    "    # fixed features about the match\n",
    "    'year','tourney_level','surface','draw_size','best_of','round',\n",
    "    'seed','entry','rank_points', 'rank_bin','opp_rank_points','opp_rank_bin','age',\n",
    "    # features to be simulated - bounded percents trunc_normal\n",
    "    'first_in_per','first_won_per','second_won_per',\n",
    "    'rally_svptw_per','ace_per','df_per',\n",
    "    'first_rpw_per','second_rpw_per','rally_rpw_per',\n",
    "    # converted from counts to percentage so trunc normal\n",
    "    'bp_face_freq','ace_per_against','bp_create_freq'    \n",
    "]\n",
    "\n",
    "trunc_normal_vars = [\n",
    "    'first_in_per','first_won_per','second_won_per',\n",
    "#     'hold_per',\n",
    "    'rally_svptw_per','ace_per','df_per',\n",
    "#     'break_per',\n",
    "    'first_rpw_per',\n",
    "    'second_rpw_per','rally_rpw_per','bp_face_freq','ace_per_against',\n",
    "    'bp_create_freq'\n",
    "]\n",
    "\n",
    "\n",
    "# add dummy variables\n",
    "model_df = pd.get_dummies(\n",
    "    player_matches_wfeat.loc[\n",
    "    :,\n",
    "    model_feats\n",
    "],\n",
    "    columns = ['surface','tourney_level','draw_size','best_of',\n",
    "              'round','seed','entry','rank_bin','opp_rank_bin'],\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "\n",
    "# keep 2023 for testing\n",
    "holdout_data = model_df.query(\n",
    "    'year == 2023'\n",
    ")\n",
    "model_df = model_df.query('year != 2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a6810",
   "metadata": {},
   "source": [
    "# Stabilization of Career Statistics\n",
    "This is for code considering the R^2 of match statistics over a player's career."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order matches based on round\n",
    "def update_tourney_date(tourney_date,round_match):\n",
    "    if round_match in ['BR','F']:\n",
    "        out = str(tourney_date) + '7'\n",
    "    elif round_match == 'SF':\n",
    "        out = str(tourney_date) + '6'\n",
    "    elif round_match == 'QF':\n",
    "        out = str(tourney_date) + '5'\n",
    "    elif round_match == 'R16':\n",
    "        out = str(tourney_date) + '4'\n",
    "    elif round_match == 'R32':\n",
    "        out = str(tourney_date) + '3'\n",
    "    elif round_match == 'R64':\n",
    "        out = str(tourney_date) + '2'\n",
    "    elif round_match == 'R128':\n",
    "        out = str(tourney_date) + '1'\n",
    "    return int(out)\n",
    "\n",
    "\n",
    "pm_with_matchno = player_matches_wfeat.query('round not in [\"RR\",\"ER\"]').assign(\n",
    "    tourney_date = lambda df: list(\n",
    "        map(\n",
    "            lambda x,y: update_tourney_date(x,y),\n",
    "            df['tourney_date'],\n",
    "            df['round']\n",
    "        )\n",
    "    )\n",
    ").groupby(\n",
    "    'name',\n",
    "    as_index = False\n",
    ").apply( # get only players whose first match was in at least 2006 and played at least 50 matches\n",
    "    lambda df: df if min(df.year) >= 2006 and df.shape[0]>=50 else None\n",
    ").reset_index().drop(\n",
    "    ['level_0','level_1'],\n",
    "    axis = 1\n",
    ").sort_values(['name','tourney_date']).loc[\n",
    "    :,\n",
    "    ['name','tourney_date','round',*trunc_normal_vars]\n",
    "].groupby(\n",
    "    'name',\n",
    "    as_index = False\n",
    ").apply(\n",
    "    lambda df: df.assign(# get match number\n",
    "        match_no = lambda x: x['tourney_date'].rank()\n",
    "    )\n",
    ").reset_index().drop(\n",
    "    ['level_0','level_1'],\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "\n",
    "stat_correlations = pm_with_matchno.pivot_longer(\n",
    "    index = ['name','tourney_date','round','match_no'],\n",
    "    names_to = 'stat_name',\n",
    "    values_to = 'value'\n",
    ").sort_values(['name','stat_name','match_no']).groupby(\n",
    "    ['name','stat_name'],\n",
    "    as_index = False\n",
    ").apply(\n",
    "    lambda df: df.assign( # get cumulative average of each statistic\n",
    "        cum_avg_value = lambda x: x['value'].expanding(1).mean(),\n",
    "        fin_value = lambda x: x['cum_avg_value'].tolist()[-1]\n",
    "    )\n",
    ").reset_index().drop(\n",
    "    ['level_0','level_1'],\n",
    "    axis = 1\n",
    ").groupby(\n",
    "    ['match_no','stat_name'],\n",
    "    as_index = False\n",
    ").apply( # make sure there is at least 1 match for a given match number\n",
    "    lambda df: df if df.shape[0] > 1 else None\n",
    ").groupby(\n",
    "    ['match_no','stat_name'],\n",
    "    as_index = False\n",
    ").apply(\n",
    "    lambda df: df.assign( # r^2 for cumulative average and final value\n",
    "        corr = lambda x: np.corrcoef(x['cum_avg_value'],x['fin_value'])[0,1]**2,\n",
    "        num_players = df.shape[0]\n",
    "    ).distinct(['stat_name','match_no','corr','num_players'])\n",
    ").reset_index().drop(\n",
    "    ['level_0','level_1'],\n",
    "    axis = 1\n",
    ").query('num_players>=104').pivot_wider(\n",
    "    index = ['match_no','num_players'],\n",
    "    names_from = 'stat_name',\n",
    "    values_from = 'corr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee27348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single R-squared plot for first_won_per\n",
    "ax = plt.plot(stat_correlations['first_won_per'],color = \"#5C8FA3\")\n",
    "x_loc_of_vline = [stat_correlations['first_won_per']>=0.8][0].tolist().index(True)+1\n",
    "plt.vlines(x = x_loc_of_vline,ymin = 0,ymax = 1,color = 'black',linestyles = 'dashed')\n",
    "plt.ylim((0,1))\n",
    "plt.title(\"First Serve Won % R-squared Cumulative vs Career\")\n",
    "plt.xlabel(\"Match Number\")\n",
    "plt.ylabel(\"R-squared\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e97944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared plots for all inputs\n",
    "fig,axs = plt.subplots(3,4,figsize = (20,10),sharex = False)\n",
    "plt.setp(axs,ylim = (0,1))\n",
    "\n",
    "fig.subplots_adjust(top = 0.99, bottom=0.01)\n",
    "fig.subplots_adjust(hspace = 0.4)\n",
    "\n",
    "colors = [\"#79BCD6\", \"#798DD6\", \"#9379D6\", \"#C279D6\", \n",
    "          \"#D679BC\", \"#D6798D\", \"#D69379\", \"#D6C279\", \n",
    "          \"#BCD679\", \"#8DD679\", \"#79D693\", \"#79D6C2\"]\n",
    "\n",
    "# plot all lines\n",
    "count = 0\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        axs[i,j].plot(stat_correlations[trunc_normal_vars[count]],color = colors[count])\n",
    "        x_loc_of_vline = [stat_correlations[trunc_normal_vars[count]]>=0.8][0].tolist().index(True)+1\n",
    "        axs[i,j].vlines(x = x_loc_of_vline,ymin = 0,ymax = 1,color = 'black',linestyles = 'dashed')\n",
    "        axs[i,j].set_title(trunc_normal_vars[count])\n",
    "        count+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48499b1a",
   "metadata": {},
   "source": [
    "# Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution for first_won_per\n",
    "ax = player_matches_wfeat['first_won_per'].hist(bins = 30, color = \"#5C8FA3\")\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1))\n",
    "ax.grid(False)\n",
    "plt.title(\"Distribution of First Serve Won %\")\n",
    "plt.xlim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions for all inputs\n",
    "fig,axs = plt.subplots(3,4,figsize = (20,10),sharex = False)\n",
    "plt.setp(axs,xlim = (0,1))\n",
    "\n",
    "\n",
    "fig.subplots_adjust(top = 0.99, bottom=0.01)\n",
    "fig.subplots_adjust(hspace = 0.4)\n",
    "\n",
    "colors = [\"#79BCD6\", \"#798DD6\", \"#9379D6\", \"#C279D6\", \n",
    "          \"#D679BC\", \"#D6798D\", \"#D69379\", \"#D6C279\", \n",
    "          \"#BCD679\", \"#8DD679\", \"#79D693\", \"#79D6C2\"]\n",
    "\n",
    "# plot all distributions\n",
    "count = 0\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        axs[i,j].hist(player_matches_wfeat[trunc_normal_vars[count]],bins = 30,color = colors[count])\n",
    "        axs[i,j].set_title(trunc_normal_vars[count])\n",
    "        axs[i,j].xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1))\n",
    "        count+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions for hold and break percentage\n",
    "fig,axs = plt.subplots(1,2,figsize = (20,10))\n",
    "plt.setp(axs,xlim = (0,1))\n",
    "\n",
    "axs[0].hist(player_matches_wfeat['hold_per'],bins = 30,color = \"#79BCD6\")\n",
    "axs[0].set_title('hold_per')\n",
    "axs[0].xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1))\n",
    "\n",
    "axs[1].hist(player_matches_wfeat['break_per'],bins = 30,color = \"#D69379\")\n",
    "axs[1].set_title('break_per')\n",
    "axs[1].xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2f4ab",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "This section includes code used to build the post-match win probability models with subsections for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = sklearn.model_selection.train_test_split(\n",
    "    model_df.drop(['result'],axis = 1),\n",
    "    model_df['result'],\n",
    "    test_size=0.2,\n",
    "    random_state = 33 # for reproducibility\n",
    ")\n",
    "\n",
    "random_seed = 54\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2236eb2c",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92adb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale values for nn, only use training data\n",
    "scaler = preprocessing.StandardScaler().fit(\n",
    "    X_train.loc[\n",
    "        :,\n",
    "        [*trunc_normal_vars,'year','age',\n",
    "         'rank_points','opp_rank_points']\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create standardized copies of data for neural networks\n",
    "X_train_nn = X_train.copy()\n",
    "X_test_nn = X_test.copy()\n",
    "\n",
    "X_train_nn.loc[:,scaler.feature_names_in_] = scaler.transform(\n",
    "    X_train_nn.loc[:,scaler.feature_names_in_]\n",
    ")\n",
    "\n",
    "X_test_nn.loc[:,scaler.feature_names_in_] = scaler.transform(\n",
    "    X_test_nn.loc[:,scaler.feature_names_in_]\n",
    ")\n",
    "\n",
    "# create dataloaders for train and validation datasets\n",
    "np.random.seed(random_seed)\n",
    "X_train_nn_tens = torch.tensor(X_train_nn.iloc[:,1:].values).float()\n",
    "y_train_nn = torch.tensor(pd.get_dummies(y_train).values).float() # make this two-dimensional for cross-entropy loss\n",
    "X_val_nn_tens = torch.tensor(X_test_nn.iloc[:,1:].values).float()\n",
    "y_val_nn = torch.tensor(pd.get_dummies(y_test).values).float()\n",
    "\n",
    "nn_train = torch.utils.data.TensorDataset(X_train_nn_tens,y_train_nn)\n",
    "train_loader = torch.utils.data.DataLoader(nn_train,batch_size = 32,shuffle=True)\n",
    "nn_val = torch.utils.data.TensorDataset(X_val_nn_tens,y_val_nn)\n",
    "val_loader = torch.utils.data.DataLoader(nn_val,batch_size = 32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self,num_hidden_layers,num_neurons,num_outputs = 2,lr = 0.05):\n",
    "        # call constructor of parent class\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.net = nn.Sequential()\n",
    "        \n",
    "        # add hidden layers\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.net.add_module('layer'+str(i),nn.LazyLinear(num_neurons[i]))\n",
    "            self.net.add_module('relu'+str(i),nn.ReLU())\n",
    "            self.net.add_module('dropout'+str(i),nn.Dropout(.25))\n",
    "           \n",
    "        # add output layer\n",
    "        self.net.add_module('output',nn.LazyLinear(num_outputs))   \n",
    "        \n",
    "    # define forward method to calculate output\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    # cross-entropy loss for two classes\n",
    "    def loss(self,y_hat,y):\n",
    "        return F.cross_entropy(y_hat,y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), self.lr)\n",
    "    \n",
    "    # predict method to output probabilities\n",
    "    def predict(self,x):\n",
    "        return nn.Softmax(dim=1)(self.net(x))\n",
    "        \n",
    "    # method to get accuracy on a batch\n",
    "    def get_accuracy(self,batch):\n",
    "        preds = self.predict(batch[0]).argmax(axis = 1)\n",
    "        actual = batch[1].argmax(axis = 1)\n",
    "        return sum(preds == actual)/len(preds == actual)\n",
    "        \n",
    "    # calculate loss for training step\n",
    "    def training_step(self,batch):\n",
    "        out_loss = self.loss(self(batch[0]),batch[1])\n",
    "        return out_loss\n",
    "    \n",
    "    # calculate loss for validation step\n",
    "    def val_step(self,batch):\n",
    "        out_loss = self.loss(self(batch[0]),batch[1])\n",
    "        return out_loss\n",
    "    \n",
    "class MyTrainer:\n",
    "    \n",
    "    def __init__(self,num_epochs,early_stopping = None):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "    def fit(self,model,train_dataloader,val_dataloader,plot_loss = False):\n",
    "        # assign things\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        \n",
    "        # get optimizer\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        \n",
    "        \n",
    "        # attributes for plotting\n",
    "        self.epoch_list = []\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "        if plot_loss: # not implemented/working\n",
    "            # first stuff for plotting\n",
    "            plt.ion()\n",
    "            fig,ax = plt.subplots(1,1)\n",
    "            fig.show()\n",
    "            fig.canvas.draw()\n",
    "        \n",
    "        for self.epoch in range(self.num_epochs): # iterate through epoch \n",
    "            \n",
    "            self.best_loss = np.inf\n",
    "            \n",
    "            # early stopping if val_loss hasn't improved after x rounds\n",
    "            if isinstance(self.early_stopping,int) and self.epoch >= self.early_stopping:\n",
    "                # starting point needs to be early_stopping+1 back so to compare the following early_stopping points\n",
    "                start_point = self.epoch-self.early_stopping-1\n",
    "                end_point = self.epoch+1\n",
    "                should_stop = [self.val_loss_list[start_point] < i for i in self.val_loss_list[(start_point+1):end_point]]\n",
    "                if sum(should_stop)==len(should_stop):\n",
    "                    print(f'Early stopping, validation loss has not improved after {self.early_stopping} rounds')\n",
    "                    break\n",
    "                \n",
    "            self.fit_epoch()\n",
    "            self.epoch_list.append(self.epoch+1)\n",
    "            \n",
    "            if plot_loss: # not implemented/working\n",
    "                # plot updates\n",
    "                ax.clear()\n",
    "                ax.plot(self.epoch_list,self.train_loss_list,label = 'train_loss',color = 'blue')\n",
    "                ax.plot(self.epoch_list,self.val_loss_list,label = 'val_loss',color = 'orange')\n",
    "                ax.plot(self.epoch_list,self.val_acc_list,label = 'val_acc',color = 'green')\n",
    "                ax.legend(loc = 'best')\n",
    "                fig.canvas.draw()\n",
    "                plt.pause(.1)\n",
    "        \n",
    "        # plot progression at end\n",
    "        fig,ax = plt.subplots(1,1)\n",
    "        ax.plot(self.epoch_list,self.train_loss_list,label = 'train_loss',color = 'blue')\n",
    "        ax.plot(self.epoch_list,self.val_loss_list,label = 'val_loss',color = 'orange')\n",
    "        ax.plot(self.epoch_list,self.val_acc_list,label = 'val_acc',color = 'green')\n",
    "        ax.legend(loc = 'best')\n",
    "        ax.set_title('Train and Val Loss')\n",
    "            \n",
    "    # feed forward and backpropogation process\n",
    "    def fit_epoch(self):\n",
    "        cum_train_loss = []\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.training_step(batch)\n",
    "            self.optim.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "            cum_train_loss.append(loss.detach())\n",
    "        if self.val_dataloader is None:\n",
    "            return\n",
    "        cum_val_loss = []\n",
    "        cum_val_acc = []\n",
    "        self.model.eval()\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                self.model.val_step(batch)\n",
    "            cum_val_loss.append(self.model.val_step(batch).detach())\n",
    "            cum_val_acc.append(self.model.get_accuracy(batch))\n",
    "        \n",
    "        # for early stopping\n",
    "        if self.model.val_step(batch).detach() < self.best_loss:\n",
    "            torch.save(self.model.state_dict(),'Final Models/MLP Best Val Tune (2024.05.28 NS).pkl')\n",
    "            # for debugging\n",
    "            self.best_tune_params = self.model.parameters()\n",
    "        \n",
    "        # store all loss and acc for plotting\n",
    "        self.train_loss_list.append(np.mean(cum_train_loss))\n",
    "        self.val_loss_list.append(np.mean(cum_val_loss))\n",
    "        self.val_acc_list.append(np.mean(cum_val_acc))\n",
    "        \n",
    "        print(f'epoch {self.epoch+1} - train_loss: {np.mean(cum_train_loss):.4f}, val_loss: {np.mean(cum_val_loss):.4f}, val_acc: {np.mean(cum_val_acc):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model with one hidden layer\n",
    "np.random.seed(random_seed)\n",
    "test_mlp = MyMLP(num_hidden_layers = 1,num_neurons = [33],lr = 0.01)\n",
    "trainer = MyTrainer(num_epochs=250,early_stopping=20)\n",
    "trainer.fit(test_mlp, train_dataloader=train_loader,val_dataloader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad0969",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c71484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(random_seed)\n",
    "# initial random forest\n",
    "rf_init = RandomForestClassifier(\n",
    "    n_estimators = 500,\n",
    "    criterion = 'gini',\n",
    "    max_depth = 25, # default none\n",
    "    verbose = 0\n",
    ")\n",
    "rf_init.fit(\n",
    "    X_train.iloc[:,1:],y_train\n",
    ")\n",
    "\n",
    "\n",
    "# grid search cross-validation\n",
    "rf_param_grid = {\n",
    "    'n_estimators':[100,200,300,400,500],\n",
    "    'max_depth':[5,10,15,20,25]\n",
    "}\n",
    "\n",
    "rf_gs = sklearn.model_selection.GridSearchCV(rf_init,rf_param_grid,cv = 5)\n",
    "rf_gs.fit(\n",
    "    X_train.iloc[:,1:],\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5844399",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# build initial model\n",
    "xgb_model_sklearn_init = xgb.XGBRegressor(\n",
    "    max_depth = 5,\n",
    "    n_estimators = 400,\n",
    "    eta = 0.1,\n",
    "    objective = 'binary:logistic',\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "xgb_model_sklearn_init.fit(\n",
    "    X_train.iloc[:,1:],\n",
    "    y_train\n",
    ")\n",
    "\n",
    "# get accuracy of initial model\n",
    "preds = xgb_model_sklearn_init.predict(X_test.iloc[:,1:])\n",
    "sklearn.metrics.accuracy_score(\n",
    "    y_pred= [round(i) for i in preds],\n",
    "    y_true = y_test\n",
    ")\n",
    "\n",
    "# tuning the hyperparameters with grid search and cross-validation\n",
    "param_grid = {\n",
    "    'max_depth':list(range(5,9)),\n",
    "    'n_estimators':[400,500,600],\n",
    "    'eta':[0.1,0.2,0.3],\n",
    "    'objective':['binary:logistic'],\n",
    "    'eval_metric':['logloss']\n",
    "}\n",
    "xgb_gs = sklearn.model_selection.GridSearchCV(xgb_model_sklearn_init,param_grid,cv = 5)\n",
    "xgb_gs.fit(\n",
    "    X_train.iloc[:,1:],\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3964b",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "During the project, it was easier to save the parameters of the distribution in a csv and load them in to parse into distributions as a method of checkpointing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64509dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get parameters of maximum likelihood distribution\n",
    "def get_trunc_normal_dist(list_of_vals,is_dominance_ratio = \"No\"):\n",
    "    mu_int = np.arange(\n",
    "        np.median(list_of_vals)-.1,\n",
    "        np.median(list_of_vals)+.1,\n",
    "        .01\n",
    "    )\n",
    "    sigma_base = np.arange(.01,0.3,.01)\n",
    "    int_param_grid = [[i,j] for i in mu_int for j in sigma_base]\n",
    "    neg_log_lik_list = [\n",
    "        log_lik_trunc_normal(\n",
    "            data=list_of_vals,\n",
    "            mu_test=i[0],\n",
    "            sigma_int = i[1]\n",
    "        ) for i in int_param_grid\n",
    "    ]\n",
    "    \n",
    "    win_index_int = (neg_log_lik_list == min(neg_log_lik_list)).tolist().index(True)\n",
    "    \n",
    "    return int_param_grid[win_index_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47070010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player distributions for truncated normal stats\n",
    "player_distributions = player_matches_wfeat.merge(\n",
    "    holdout_data.loc[:,['match_id','result']].assign(flag = 1),\n",
    "    how = 'left'\n",
    ").query(\n",
    "    '@pd.isna(flag)'\n",
    ").drop('flag',axis = 1).groupby(\n",
    "    ['name','surface'],\n",
    "    as_index = False\n",
    ").aggregate(\n",
    "    {col:((lambda x: list(x)) if col in trunc_normal_vars else len) for col in [*trunc_normal_vars,'tourney_name']}\n",
    ").rename(\n",
    "    columns = {'tourney_name':'num_matches'}\n",
    ").query(\n",
    "    'num_matches >= 75 or (num_matches >= 60 and surface == \"Grass\")'\n",
    ").pivot_longer(\n",
    "    index = ['name','surface','num_matches'],\n",
    "    names_to = 'stat_name',\n",
    "    values_to = 'value'\n",
    ").assign(\n",
    "    dist_params = lambda df: list(\n",
    "        map(\n",
    "            get_trunc_normal_dist,\n",
    "            df['value']\n",
    "        )\n",
    "    )\n",
    ").pivot(\n",
    "    columns = 'stat_name',\n",
    "    index = ['name','num_matches','surface'],\n",
    "    values = 'dist_params'\n",
    ").reset_index()\n",
    "player_distributions.to_csv(\"Player Distributions (2024.04.15 NS).csv\",index = False)\n",
    "\n",
    "# reload in player distributions and parse into distributions\n",
    "player_distributions = pd.read_csv(\"Player Distributions (2024.04.15 NS).csv\").apply(\n",
    "    lambda df: df if df.name not in trunc_normal_vars else list(\n",
    "        map(\n",
    "            lambda x: x.strip('][').split(','),\n",
    "            df\n",
    "        )\n",
    "    )\n",
    ").apply(\n",
    "    lambda df: df if df.name not in trunc_normal_vars else list(\n",
    "        map(\n",
    "            lambda x: trunc_normal(0,1,float(x[0]),float(x[1])),\n",
    "            df\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5153656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions for each rank bin against other rank bins\n",
    "rank_bin_distributions = player_matches_wfeat.merge(\n",
    "    holdout_data.loc[:,['match_id','result']].assign(flag = 1),\n",
    "    how = 'left'\n",
    ").query(\n",
    "    '@pd.isna(flag)'\n",
    ").drop('flag',axis = 1).groupby(\n",
    "    ['rank_bin','opp_rank_bin','surface'],\n",
    "    as_index = False\n",
    ").aggregate(\n",
    "    {col:((lambda x: list(x)) if col in trunc_normal_vars else len) for col in [*trunc_normal_vars,'tourney_name']}\n",
    ").rename(\n",
    "    columns = {'tourney_name':'num_matches'}\n",
    ").pivot_longer(\n",
    "    index = ['rank_bin','opp_rank_bin','surface','num_matches'],\n",
    "    names_to = 'stat_name',\n",
    "    values_to = 'value'\n",
    ").assign(\n",
    "    dist_params = lambda df: list(\n",
    "        map(\n",
    "            get_trunc_normal_dist,\n",
    "            df['value']\n",
    "        )\n",
    "    )\n",
    ").pivot(\n",
    "    columns = 'stat_name',\n",
    "    index = ['rank_bin','opp_rank_bin','num_matches','surface'],\n",
    "    values = 'dist_params'\n",
    ").reset_index()\n",
    "rank_bin_distributions.to_csv(\"Rank Bin Distributions (2024.04.15 NS).csv\",index = False)\n",
    "\n",
    "\n",
    "rank_bin_distributions = pd.read_csv(\"Rank Bin Distributions (2024.04.15 NS).csv\").apply(\n",
    "    lambda df: df if df.name not in trunc_normal_vars else list(\n",
    "        map(\n",
    "            lambda x: x.strip('][').split(','),\n",
    "            df\n",
    "        )\n",
    "    )\n",
    ").apply(\n",
    "    lambda df: df if df.name not in trunc_normal_vars else list(\n",
    "        map(\n",
    "            lambda x: trunc_normal(0,1,float(x[0]),float(x[1])),\n",
    "            df\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f81975",
   "metadata": {},
   "source": [
    "# Evaluating the Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afa5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline naive predictions based on rank\n",
    "naive_preds = holdout_data.assign(\n",
    "    pred_result = lambda df: list(\n",
    "        map(\n",
    "            lambda x,y:1 if x>y else 0,\n",
    "            df['rank_points'],\n",
    "            df['opp_rank_points']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "naive_rank_acc = sklearn.metrics.accuracy_score(\n",
    "    y_pred= round(naive_preds['pred_result']),\n",
    "    y_true = naive_preds['result']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return accuracy from num_sims, differs a bit for neural network\n",
    "def eval_num_sims(num_sims,model_param,is_nn = False):\n",
    "    np.random.seed(random_seed)\n",
    "    # join distributions then run sims\n",
    "    holdout_sim_data_int = pd.concat(\n",
    "        [\n",
    "            holdout_data.merge(# player distributions\n",
    "                player_matches_wfeat.loc[:,['match_id','result','surface','name']],\n",
    "                how = 'left'\n",
    "            ).drop(trunc_normal_vars,axis = 1).merge(\n",
    "                player_distributions.drop(['hold_per','break_per'],axis = 1),\n",
    "                how = 'left'\n",
    "            ).query(\n",
    "                \"not @pd.isna(first_in_per)\"\n",
    "            ),\n",
    "            holdout_data.merge(# rank distributions, only join if no player distribution match\n",
    "                player_matches_wfeat.loc[:,['match_id','result','surface','name','rank_bin','opp_rank_bin']],\n",
    "                how = 'left'\n",
    "            ).drop(trunc_normal_vars,axis = 1).merge(\n",
    "                player_distributions.drop(['hold_per','break_per'],axis = 1),\n",
    "                how = 'left'\n",
    "            ).query(\n",
    "                \"@pd.isna(first_in_per)\"\n",
    "            ).drop(trunc_normal_vars,axis = 1).merge(\n",
    "                rank_bin_distributions.drop(['hold_per','break_per'],axis = 1),\n",
    "                how = 'left',\n",
    "                on = ['rank_bin','opp_rank_bin','surface']\n",
    "            )\n",
    "        ]\n",
    "    ).apply(\n",
    "        lambda df: df if df.name not in trunc_normal_vars else list(\n",
    "            map( # get simulations using rvs method\n",
    "                lambda x: x.rvs(num_sims),\n",
    "                df\n",
    "            )\n",
    "        )\n",
    "    ).explode(trunc_normal_vars).apply(\n",
    "        lambda df: df if df.name not in trunc_normal_vars else df.astype(float)\n",
    "    )\n",
    "    \n",
    "    # standardize columns if using neural network\n",
    "    if is_nn:\n",
    "        holdout_sim_data_int.loc[:,scaler.feature_names_in_] = scaler.transform(\n",
    "            holdout_sim_data_int.loc[:,scaler.feature_names_in_]\n",
    "        )\n",
    "        preds_int = model_param.predict(\n",
    "            torch.tensor(\n",
    "                holdout_sim_data_int.loc[\n",
    "                    :,\n",
    "                    X_train.columns[1:]\n",
    "                ].values\n",
    "            ).float()\n",
    "        ).argmax(axis = 1).detach().numpy()\n",
    "    else:\n",
    "         # get predictions for simulations\n",
    "        preds_int = model_param.predict(\n",
    "            holdout_sim_data_int.loc[\n",
    "                :,\n",
    "                X_train.columns[1:]\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    holdout_sim_data_int['pred_result'] = preds_int\n",
    "    \n",
    "    # for each match, get average predicted results over the simulations\n",
    "    actual_preds_int = holdout_sim_data_int.groupby(\n",
    "        ['match_id','result'],\n",
    "        as_index = False\n",
    "    ).aggregate(\n",
    "        pred_result = ('pred_result',lambda x: sum(round(x))/len(x))\n",
    "    )\n",
    "    \n",
    "    # normalize the predictions for the match\n",
    "    further_analysis_int = player_matches_wfeat.query(\n",
    "        'year == 2023'\n",
    "    ).loc[\n",
    "        :,\n",
    "        ['match_id','name','tourney_name','round','result']\n",
    "    ].merge(\n",
    "        actual_preds_int.loc[:,['match_id','pred_result','result']]\n",
    "    ).sort_values(\n",
    "        'match_id'\n",
    "    ).merge(\n",
    "        actual_preds_int.groupby('match_id',as_index = False).aggregate(tot_result = ('pred_result',sum))\n",
    "    ).assign(\n",
    "        norm_pred_result = lambda df: df['pred_result']/df['tot_result']\n",
    "    )\n",
    "    \n",
    "    acc = sklearn.metrics.accuracy_score(\n",
    "        y_pred= round(further_analysis_int['norm_pred_result']),\n",
    "        y_true = further_analysis_int['result']\n",
    "    )\n",
    "    \n",
    "    print(f'done with {num_sims} sims')\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1134d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy for number of sims across the three models\n",
    "sim_testing_xgb = pd.DataFrame(\n",
    "    {\n",
    "        'num_sims':[100,300,500,750,1000,2000,3000]\n",
    "    }\n",
    ").assign(\n",
    "    accuracy_sims = lambda df: list(\n",
    "        map(\n",
    "            lambda x: eval_num_sims(x,xgb_gs),\n",
    "            df.num_sims\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print('xgb done')\n",
    "sim_testing_rf = pd.DataFrame(\n",
    "    {\n",
    "        'num_sims':[100,300,500,750,1000,2000,3000]\n",
    "    }\n",
    ").assign(\n",
    "    accuracy_sims = lambda df: list(\n",
    "        map(\n",
    "            lambda x: eval_num_sims(x,rf_gs),\n",
    "            df.num_sims\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# print('rf done')\n",
    "sim_testing_nn = pd.DataFrame(\n",
    "    {\n",
    "        'num_sims':[100,300,500,750,1000,2000,3000]\n",
    "    }\n",
    ").assign(\n",
    "    accuracy_sims = lambda df: list(\n",
    "        map(\n",
    "            lambda x: eval_num_sims(x,final_mlp,is_nn=True),\n",
    "            df.num_sims\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print('nn done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dac6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance of three models over different number of sims\n",
    "fig,ax = plt.subplots()\n",
    "x_loc = np.arange(7)\n",
    "col_width = 0.15\n",
    "\n",
    "# plot three models\n",
    "ax.bar(x_loc,sim_testing_nn['accuracy_sims'],color = '#5C8FA3',width = col_width,label = 'MLP')\n",
    "ax.bar(x_loc+col_width,sim_testing_rf['accuracy_sims'],color = '#A35C8F',width = col_width,label = 'RF')\n",
    "ax.bar(x_loc+2*col_width,sim_testing_xgb['accuracy_sims'],color = '#8FA35C',width = col_width,label = 'XGBoost')\n",
    "\n",
    "plt.xticks(ticks = x_loc + col_width,labels=sim_testing_rf['num_sims'])\n",
    "plt.ylim((0.60,0.66))\n",
    "plt.title('Accuracy of Model Predictions vs Naive Ranking Approach')\n",
    "plt.xlabel('Number of Simulations')\n",
    "plt.ylabel('Accuracy')\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# add naive stuff\n",
    "plt.hlines(y =naive_rank_acc,xmin = -0.5,xmax = 7.35,color = '#C79038')\n",
    "plt.text(6.5,naive_rank_acc+.0005,str(round(naive_rank_acc*100,2))+'%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef624394",
   "metadata": {},
   "source": [
    "## Extra analysis by surface and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1534b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw 100 random samples for analysis by surface and number of players with distribution\n",
    "np.random.seed(random_seed)\n",
    "holdout_sim_data = pd.concat(\n",
    "    [\n",
    "        holdout_data.merge(# player distributions\n",
    "            player_matches_wfeat.loc[:,['match_id','result','surface','name']],\n",
    "            how = 'left'\n",
    "        ).drop(trunc_normal_vars,axis = 1).merge(\n",
    "            player_distributions.drop(['hold_per','break_per'],axis = 1),\n",
    "            how = 'left'\n",
    "        ).query(\n",
    "            \"not @pd.isna(first_in_per)\"\n",
    "        ),\n",
    "        holdout_data.merge(# rank distributions\n",
    "            player_matches_wfeat.loc[:,['match_id','result','surface','name','rank_bin','opp_rank_bin']],\n",
    "            how = 'left'\n",
    "        ).drop(trunc_normal_vars,axis = 1).merge(\n",
    "            player_distributions.drop(['hold_per','break_per'],axis = 1),\n",
    "            how = 'left'\n",
    "        ).query(\n",
    "            \"@pd.isna(first_in_per)\"\n",
    "        ).drop(trunc_normal_vars,axis = 1).merge(\n",
    "            rank_bin_distributions.drop(['hold_per','break_per'],axis = 1),\n",
    "            how = 'left',\n",
    "            on = ['rank_bin','opp_rank_bin','surface']\n",
    "        )\n",
    "    ]\n",
    ").apply(\n",
    "    lambda df: df if df.name not in trunc_normal_vars else list(\n",
    "        map( # get simulations using rvs method\n",
    "            lambda x: x.rvs(100),\n",
    "            df\n",
    "        )\n",
    "    )\n",
    ").explode(trunc_normal_vars).apply(\n",
    "    lambda df: df if df.name not in trunc_normal_vars else df.astype(float)\n",
    ")\n",
    "\n",
    "# get predicted results using random forest model\n",
    "preds_hold = rf_gs.predict(\n",
    "    holdout_sim_data.loc[\n",
    "        :,\n",
    "        X_train.columns[1:]\n",
    "    ]\n",
    ")\n",
    "    \n",
    "preds_hold = np.array(\n",
    "    [round(i) for i in preds_hold]\n",
    ")\n",
    "\n",
    "holdout_sim_data['pred_result'] = preds_hold\n",
    "act_pred_data = holdout_sim_data.groupby(\n",
    "    ['match_id','result'],\n",
    "    as_index = False\n",
    ").aggregate(\n",
    "    pred_result = ('pred_result',lambda x: sum(round(x))/len(x))\n",
    ")\n",
    "\n",
    "\n",
    "# normalize the probabilities from random forest results\n",
    "norm_wp = player_matches_wfeat.query(\n",
    "    'year == 2023'\n",
    ").loc[\n",
    "    :,\n",
    "    ['match_id','name','tourney_name','surface','round','result']\n",
    "].merge(\n",
    "    act_pred_data.loc[:,['match_id','pred_result','result']]\n",
    ").sort_values(\n",
    "    'match_id'\n",
    ").merge(\n",
    "    act_pred_data.groupby('match_id',as_index = False).aggregate(tot_result = ('pred_result',sum))\n",
    ").assign(\n",
    "    norm_pred_result = lambda df: df['pred_result']/df['tot_result']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d60077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model performance by surface\n",
    "sim_surface_acc = norm_wp.assign(\n",
    "    rounded_norm_prob = lambda df: round(df.norm_pred_result)\n",
    ").groupby(\n",
    "    'match_id',\n",
    "    as_index = False\n",
    ").apply(\n",
    "    lambda df: df.assign(\n",
    "        acc = lambda df2: df2.rounded_norm_prob == df2.result \n",
    "    )\n",
    ").distinct(\n",
    "    ['match_id','acc','surface']\n",
    ").groupby(\n",
    "    'surface',\n",
    "    as_index = False\n",
    ").aggregate(\n",
    "    sim_acc = ('acc',np.mean)\n",
    ")\n",
    "\n",
    "# naive predictions by surface\n",
    "naive_surface_acc = naive_preds.merge(\n",
    "    player_matches_wfeat.distinct(['match_id','surface']),\n",
    "    how='left',\n",
    "    on='match_id'\n",
    ").distinct(\n",
    "    ['match_id','surface','result','pred_result']\n",
    ").assign(\n",
    "    acc = lambda df: df.result == df.pred_result\n",
    ").distinct(['match_id','surface','acc']).groupby(\n",
    "    'surface',\n",
    "    as_index = False\n",
    ").aggregate(\n",
    "    naive_acc = ('acc',np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4254c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot simulation approach vs rank-based across surfaces\n",
    "fig,ax = plt.subplots()\n",
    "ax.bar([0,1,2],sim_surface_acc['sim_acc'],width = 0.3,label = '100 Simulations',color = '#5C8FA3')\n",
    "ax.bar([.3,1.3,2.3],naive_surface_acc['naive_acc'],width = 0.3, label = 'Rank-Based Approach',color = '#A3705C')\n",
    "plt.xticks(ticks = [0.15,1.15,2.15],labels=sim_surface_acc['surface'])\n",
    "plt.ylim((0.60,0.66))\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Accuracy')\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1))\n",
    "plt.title('Simulation Accuracy Across Surfaces')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot simulation performance based on whether the players in the match had their own distributions to draw from\n",
    "perf_by_dist_pres = norm_wp.assign(\n",
    "    rounded_norm_prob = lambda df: round(df.norm_pred_result)\n",
    ").merge(\n",
    "    player_distributions.loc[:,['name','surface']].assign(player_flag = 1),\n",
    "    how = 'left',\n",
    "    on = ['name','surface']\n",
    ").fillna(0).assign(\n",
    "    acc = lambda df: df.rounded_norm_prob == df.result\n",
    ").groupby(\n",
    "    ['match_id','acc'],\n",
    "    as_index = False\n",
    ").aggregate(\n",
    "    num_player_dist = ('player_flag',sum)\n",
    ").groupby(\n",
    "    'num_player_dist',\n",
    "    as_index = False\n",
    ").aggregate(\n",
    "    sim_acc = ('acc',np.mean)\n",
    ")\n",
    "\n",
    "# plot\n",
    "fig,ax = plt.subplots()\n",
    "ax.bar([str(int(i)) for i in perf_by_dist_pres['num_player_dist']],perf_by_dist_pres['sim_acc'],color = '#5C8FA3')\n",
    "plt.ylim((0.60,0.68))\n",
    "plt.xlabel('Number of Players with Distribution')\n",
    "plt.ylabel('Accuracy')\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1))\n",
    "plt.title('Simulation Accuracy by Number of Players with Distribution')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
